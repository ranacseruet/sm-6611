#!/usr/bin/env python

import urllib2
import re
from musette import environ
from bs4 import BeautifulSoup
import os
import sys

def getRawContent(url):
    content = urllib2.urlopen(url).read()
    #add regular expression
    return content

def readConfig(filePath):
    environ.read(filePath)
    url = environ.str("url")
    dir = environ.str("root_directory")
    return url, dir

def getGitURL(content):
    # Looking for a .git url
    regx = "\"(((git|ssh|http(s)?)|(git@[\w\.]+))(:(//)?)([\w\.@\:/\-~]+)(\.git))\""
    match = re.findall(re.compile(regx), content)
    if match:
        return match[0][0]
    return

def crawlSite(url, curLevel):
    content = getRawContent(url)
    gitURL = getGitURL(content)
    if gitURL:
        return gitURL

    if curLevel < 2:
        soup = BeautifulSoup(content)
        for link in soup.find_all('a'):
            '''Look for only github urls'''
            pattern = re.compile("^(https?:\/\/)?(github)\.([a-z\.]{2,6})([\/\w \.-]*)*\/?")
            if pattern.search(link.get('href')):
                #print link.get('href')
                gitURL = crawlSite(link.get('href'), curLevel+1)
                if gitURL:
                    return gitURL

    return False

if __name__ == '__main__':
    if int(len(sys.argv)) < 2:
        print "config file path not given"
        os._exit(0)
    #Recursive currently getting different git repos
    url, dir = readConfig(sys.argv[1])
    gitURL = crawlSite(url, 0)
    if gitURL:
        os.system("cd "+dir+" && "+"git clone "+gitURL)
    else:
        print "No repos found"
